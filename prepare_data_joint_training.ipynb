{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#/bin/bash\n",
        "\n",
        "# This script preprocesses and binarizes the data for training translation models using fairseq.\n",
        "# Only difference between this script and `prepare_data_joint_finetuning.sh` that we generate\n",
        "# fairseq dict using this script that is commonly shared across for training all the models further.\n",
        "\n",
        "%%shell\n",
        "echo `date`\n",
        "exp_dir=$1                                      # path to the experiment directory\n",
        "vocab_dir=${2:-\"$exp_dir/vocab\"}                # path to the spm-based tokenizer directory\n",
        "train_data_dir=${3:-\"$exp_dir/train\"}           # path to the train data within experiment directory\n",
        "devtest_data_dir=${4:-\"$exp_dir/devtest/all\"}   # path to the devtest data within experiment directory\n",
        "\n",
        "root=$(dirname $0)\n",
        "\n",
        "echo \"Running experiment ${exp_dir}\"\n",
        "\n",
        "train_processed_dir=$exp_dir/data\n",
        "devtest_processed_dir=$exp_dir/data\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "mkdir -p $train_processed_dir\n",
        "mkdir -p $devtest_processed_dir\n",
        "mkdir -p $out_data_dir\n",
        "\n",
        "parallel_installed=false\n",
        "\n",
        "# Check if GNU Parallel is installed\n",
        "if command -v parallel &> /dev/null; then\n",
        "    echo \"GNU Parallel is installed. Version information:\"\n",
        "    parallel --version\n",
        "    parallel_installed=true\n",
        "fi\n",
        "\n",
        "# get a list of language pairs in the `train_data_dir`\n",
        "pairs=$(ls -d $train_data_dir/* | sort)\n",
        "\n",
        "\n",
        "# iterate over each language pair\n",
        "for pair in ${pairs[@]}; do\n",
        "    # extract the source and target languages from the pair name\n",
        "    pair=$(basename $pair)\n",
        "    src_lang=$(echo \"$pair\" | cut -d \"-\" -f 1)\n",
        "    tgt_lang=$(echo \"$pair\" | cut -d \"-\" -f 2)\n",
        "    echo \"$src_lang - $tgt_lang\"\n",
        "\n",
        "    train_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\tdevtest_norm_dir=$exp_dir/norm/$src_lang-$tgt_lang\n",
        "\tmkdir -p $train_norm_dir\n",
        "\tmkdir -p $devtest_norm_dir\n",
        "\n",
        "\n",
        "    # check if the source language text requires transliteration\n",
        "    src_transliterate=\"true\"\n",
        "    if [[ $src_lang == *\"Arab\"* ]] || [[ $src_lang == *\"Olck\"* ]] || \\\n",
        "        [[ $src_lang == *\"Mtei\"* ]] || [[ $src_lang == *\"Latn\"* ]]; then\n",
        "        src_transliterate=\"false\"\n",
        "    fi\n",
        "\n",
        "    # check if the target language text requires transliteration\n",
        "    tgt_transliterate=\"true\"\n",
        "    if [[ $tgt_lang == *\"Arab\"* ]] || [[ $tgt_lang == *\"Olck\"* ]] || \\\n",
        "        [[ $tgt_lang == *\"Mtei\"* ]] || [[ $tgt_lang == *\"Latn\"* ]]; then\n",
        "        tgt_transliterate=\"false\"\n",
        "    fi\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    #                           train preprocessing\n",
        "    # --------------------------------------------------------------------------\n",
        "\ttrain_infname_src=$train_data_dir/${src_lang}-${tgt_lang}/train.$src_lang\n",
        "\ttrain_infname_tgt=$train_data_dir/${src_lang}-${tgt_lang}/train.$tgt_lang\n",
        "\ttrain_outfname_src=$train_norm_dir/train.$src_lang\n",
        "\ttrain_outfname_tgt=$train_norm_dir/train.$tgt_lang\n",
        "\n",
        "    echo \"Normalizing punctuations for train\"\n",
        "    if $parallel_installed; then\n",
        "        parallel --pipe --keep-order bash $root/normalize_punctuation.sh $src_lang < $train_infname_src > $train_outfname_src._norm\n",
        "        parallel --pipe --keep-order bash $root/normalize_punctuation.sh $tgt_lang < $train_infname_tgt > $train_outfname_tgt._norm\n",
        "    else\n",
        "        bash $root/normalize_punctuation.sh $src_lang < $train_infname_src > $train_outfname_src._norm\n",
        "        bash $root/normalize_punctuation.sh $tgt_lang < $train_infname_tgt > $train_outfname_tgt._norm\n",
        "    fi\n",
        "\n",
        "\t# add do not translate tags to handle special failure cases\n",
        "    echo \"Applying do not translate tags for train\"\n",
        "    python3 scripts/normalize_regex.py $train_outfname_src._norm $train_outfname_tgt._norm $train_outfname_src.norm $train_outfname_tgt.norm\n",
        "\n",
        "\techo \"Applying normalization and script conversion for train\"\n",
        "    # this script preprocesses the text and for indic languages, converts script to devanagari if needed\n",
        "\tinput_size=`python3 scripts/preprocess_translate.py $train_outfname_src.norm $train_outfname_src $src_lang $src_transliterate false`\n",
        "\tinput_size=`python3 scripts/preprocess_translate.py $train_outfname_tgt.norm $train_outfname_tgt $tgt_lang $tgt_transliterate true`\n",
        "\techo \"Number of sentences in train: $input_size\"\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    #                              dev preprocessing\n",
        "    # --------------------------------------------------------------------------\n",
        "\tdev_infname_src=$devtest_data_dir/${src_lang}-${tgt_lang}/dev.$src_lang\n",
        "\tdev_infname_tgt=$devtest_data_dir/${src_lang}-${tgt_lang}/dev.$tgt_lang\n",
        "\tdev_outfname_src=$devtest_norm_dir/dev.$src_lang\n",
        "\tdev_outfname_tgt=$devtest_norm_dir/dev.$tgt_lang\n",
        "\n",
        "    echo \"Normalizing punctuations for dev\"\n",
        "    if $parallel_installed; then\n",
        "        parallel --pipe --keep-order bash normalize_punctuation.sh $src_lang < $dev_infname_src > $dev_outfname_src._norm\n",
        "        parallel --pipe --keep-order bash normalize_punctuation.sh $tgt_lang < $dev_infname_tgt > $dev_outfname_tgt._norm\n",
        "    else\n",
        "        bash normalize_punctuation.sh $src_lang < $dev_infname_src > $dev_outfname_src._norm\n",
        "        bash normalize_punctuation.sh $tgt_lang < $dev_infname_tgt > $dev_outfname_tgt._norm\n",
        "    fi\n",
        "\n",
        "\t# add do not translate tags to handle special failure cases\n",
        "    echo \"Applying do not translate tags for dev\"\n",
        "    python3 scripts/normalize_regex.py $dev_outfname_src._norm $dev_outfname_tgt._norm $dev_outfname_src.norm $dev_outfname_tgt.norm\n",
        "\n",
        "    echo \"Applying normalization and script conversion for dev\"\n",
        "    # this script preprocesses the text and for indic languages, converts script to devanagari if needed\n",
        "\tinput_size=`python scripts/preprocess_translate.py $dev_outfname_src.norm $dev_outfname_src $src_lang $src_transliterate false`\n",
        "\tinput_size=`python scripts/preprocess_translate.py $dev_outfname_tgt.norm $dev_outfname_tgt $tgt_lang $tgt_transliterate true`\n",
        "\techo \"Number of sentences in dev: $input_size\"\n",
        "done\n",
        "\n",
        "\n",
        "# this concatenates lang pair data and creates text files to keep track of number of\n",
        "# lines in each lang pair. this is important for joint training, as we will merge all\n",
        "# the lang pairs and the indivitual lang lines info would be required for adding specific\n",
        "# lang tags later.\n",
        "# the outputs of these scripts will  be text file like this:\n",
        "# <lang1> <lang2> <number of lines>\n",
        "# lang1-lang2 n1\n",
        "# lang1-lang3 n2\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data 'train'\n",
        "python scripts/concat_joint_data.py $exp_dir/norm $exp_dir/data 'dev'\n",
        "\n",
        "\n",
        "# tokenization of train and dev set using the spm trained models\n",
        "mkdir -p $exp_dir/bpe\n",
        "\n",
        "splits=(train dev)\n",
        "for split in ${splits[@]}; do\n",
        "\techo \"Applying sentence piece for $split\"\n",
        "\tbash apply_sentence_piece.sh $exp_dir $exp_dir/data $exp_dir/bpe SRC TGT $split $parallel_installed\n",
        "done\n",
        "\n",
        "\n",
        "# this is only required for joint training\n",
        "# we apply language tags to the bpe segmented data\n",
        "# if we are translating lang1 to lang2 then <lang1 line> will become <lang1> <lang2> <lang1 line>\n",
        "mkdir -p $exp_dir/final\n",
        "\n",
        "echo \"Adding language tags\"\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'train'\n",
        "python scripts/add_joint_tags_translate.py $exp_dir 'dev'\n",
        "\n",
        "\n",
        "# this is important step if you are training with tpu and using num_batch_buckets\n",
        "# the current implementation does not remove outliers before bucketing and hence\n",
        "# removing these large sentences ourselves helps with getting better buckets\n",
        "# python scripts/remove_large_sentences.py $exp_dir/bpe/train.SRC $exp_dir/bpe/train.TGT $exp_dir/final/train.SRC $exp_dir/final/train.TGT\n",
        "# python scripts/remove_large_sentences.py $exp_dir/bpe/dev.SRC $exp_dir/bpe/dev.TGT $exp_dir/final/dev.SRC $exp_dir/final/dev.TGT\n",
        "# python scripts/remove_large_sentences.py $exp_dir/bpe/test.SRC $exp_dir/bpe/test.TGT $exp_dir/final/test.SRC $exp_dir/final/test.TGT\n",
        "\n",
        "\n",
        "echo \"Binarizing data\"\n",
        "\n",
        "# use cpu_count to get num_workers instead of setting it manually when running\n",
        "# in different instances\n",
        "num_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\n",
        "\n",
        "data_dir=$exp_dir/final\n",
        "out_data_dir=$exp_dir/final_bin\n",
        "\n",
        "rm -rf $out_data_dir\n",
        "\n",
        "fairseq-preprocess \\\n",
        "    --source-lang SRC --target-lang TGT \\\n",
        "    --trainpref $data_dir/train \\\n",
        "    --validpref $data_dir/dev \\\n",
        "    --destdir $out_data_dir \\\n",
        "    --workers $num_workers \\\n",
        "    --thresholdtgt 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh3siWpFN-8h",
        "outputId": "1eb7864a-07ba-4216-e881-3d7dcef81f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 31 05:56:33 AM UTC 2024\n",
            "Running experiment \n",
            "GNU Parallel is installed. Version information:\n",
            "parallel: invalid option -- '-'\n",
            "parallel [OPTIONS] command -- arguments\n",
            "\tfor each argument, run command with argument, in parallel\n",
            "parallel [OPTIONS] -- commands\n",
            "\trun specified commands in parallel\n",
            "ls: cannot access '/train/*': No such file or directory\n",
            "python3: can't open file '/content/scripts/concat_joint_data.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/scripts/concat_joint_data.py': [Errno 2] No such file or directory\n",
            "Applying sentence piece for train\n",
            "bash: apply_sentence_piece.sh: No such file or directory\n",
            "Applying sentence piece for dev\n",
            "bash: apply_sentence_piece.sh: No such file or directory\n",
            "Adding language tags\n",
            "python3: can't open file '/content/scripts/add_joint_tags_translate.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/scripts/add_joint_tags_translate.py': [Errno 2] No such file or directory\n",
            "Binarizing data\n",
            "/content/IndicTrans2-main/prepare_data_joint_training.sh: line 177: fairseq-preprocess: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_a4X4YUuO0gh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}